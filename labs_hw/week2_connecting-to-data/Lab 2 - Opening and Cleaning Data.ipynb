{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca879157",
   "metadata": {},
   "source": [
    "# Lab 2 - Opening and Cleaning Data\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/gaulinmp/AccountingDataAnalytics/blob/main/labs_hw/week2_connecting-to-data/Lab 2 - Opening and Cleaning Data.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook will walk you through the process of opening and cleaning the Journal Entry dataset. We will cover the following steps:\n",
    "\n",
    "1. Loading the data\n",
    "2. Cleaning the data\n",
    "\n",
    "The intent of the notebook is for you to execute each cell in order. Some will work, others will throw errors, and then I'll explain what's going on, and help you through it.\n",
    "If you're new to programming, it can sometimes feel daunting, so I want to provide these initial notebooks to help you get started, and hopefully show you that programming (especially in Colab) isn't all that bad once you try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a comment\n",
    "\n",
    "# The code below are called imports, which is basically telling python to load specific functionality\n",
    "# This one uses a python library called pathlib to access the Path object, which is just a way of pointing to files on the computer\n",
    "from pathlib import Path\n",
    "# This import is the real magic, pandas is the library that makes data manipulation easy (well... easier)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8038e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the file, and make sure it exists\n",
    "file_path = Path(\"JEA Detail Raw.txt\")\n",
    "file_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e52593",
   "metadata": {},
   "source": [
    "That should say True above. If it says False, then your data is not loaded correctly. Please make sure to drag the `JAE Detail.txt` file into your Colab page (see Lab 1 for instructions).\n",
    "\n",
    "Okay, now to actually load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas\n",
    "df = pd.read_csv(file_path)\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0e89c",
   "metadata": {},
   "source": [
    "Oh no! An Error!\n",
    "\n",
    "Welcome to debugging. This is often the hardest and least rewarding part of programming, because while there is often a few right ways to do something, there are infinite *wrong* ways to do something. So it's like searching for a needle in an infinite ocean of rusty needles.\n",
    "\n",
    "*Future Mac here: I wrote this lab on my local machine, then tried it out on Google Colab. Colab shows you a button that says Next Steps: Explain Error. If you click this button, it immediately comes up with the solution. Being suspicious, I tried a new notebook with just the pandas read, and it took me 6 or 7 leading questions with this button and interacting with Gemini before I got the full solution working. So in this notebook, Gemini is reading down below, sees that I've solved the problem, and passes off the solution as its own. AI is all just plagerism anyway, so maybe unsurprising? Anyway, if you want the \"authentic\" coding with AI in Colab experience, I suggest starting an empty notebook and writing the code from scratch with its help. That'll be more indicative of what you might expect in practice. And now I know that Google Colab is great for debugging much faster than with just my meat brain!*\n",
    "\n",
    "In this case, we look at the error, specifically `ParserError` and try googling for that. So I copy and paste that last line into [Gemini](https://gemini.google.com) (or [Claude](https://claude.ai) or [ChatGPT](https://chat.openai.com)), which tells me:\n",
    "\n",
    "<blockquote>\n",
    "  This is a common **Pandas** error. It usually occurs when `read_csv` tries to parse a file but finds a row that violates the structure established by the header or the first row.\n",
    "\n",
    "Specifically, \"Expected 1 fields... saw 2\" usually implies one of two things:\n",
    "\n",
    "1. **Wrong Separator:** Pandas failed to find the default separator (comma) in the first row, so it assumed the file has only 1 column. Then, on line 14, it found a character that acted like a separator.\n",
    "2. **Metadata/Junk at the top:** The file has a title or description in the first few lines that looks like a single column, confusing the parser before it hits the actual data.\n",
    "\n",
    "Here are the most effective ways to fix this.\n",
    "\n",
    "...\n",
    "</blockquote>\n",
    "\n",
    "It then suggests to try a different separater, starting with tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a3193",
   "metadata": {},
   "source": [
    "Same error, different numbers. What's happening now? Well the error seems to say it expected 10 fields, but saw 37. \n",
    "\n",
    "Okay, let's look at the data (we should have started here, but we're eager) (also note that exclamation mark `!head` command in the next line, that's not Python. Then what *is* it? I suggest you ask the internet to find out this lovely little bit of Notebook tech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head \"{file_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79faf0",
   "metadata": {},
   "source": [
    "Uh... that's ugly. What's going on? Well, a few things. Playing with raw data is hard, Excel, Tableau, Alteryx, etc. have put a lot of code into trying to figure out things like this. But in Python, we'd have to do it ourselves. So we can see that there's a bunch of text that isn't the actual file we want. In fact, we can't even see the data we expect. Let look at more lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b47e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 20 \"{file_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba49e1c",
   "metadata": {},
   "source": [
    "Well there's the data, finally. So we're closer. But there's a bunch of random text above the table we want, and those headers (line with `Account \"Account`) are ugly and splits the column names across two lines. That means that we need to *a)* skip the first 8 lines (you can count to verify), and *b)* strip newlines from the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas\n",
    "df = pd.read_csv(file_path, sep='\\t', thousands=',', skiprows=8)\n",
    "# Rename the columns, using a trick to remove extra spaces and newlines. Essentially, we split each column name on whitespace, then re-join with a single space, and strip any leading/trailing spaces.\n",
    "df = df.rename(columns={c:' '.join(c.split()).strip() for c in df.columns})\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326aee0",
   "metadata": {},
   "source": [
    "Now let's drop the three columns at the end that are entirely empty. That's what the `how='all'` means. `axis=1` means check columns, `axis=0` would check the rows for all missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb70c9",
   "metadata": {},
   "source": [
    "Now let's remove newlines (`\\n`) from the user names. While you're at it, [look up](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.html) what `.str` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2eb0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"User ID\"] = df[\"User ID\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653172ae",
   "metadata": {},
   "source": [
    "Let's look at the cleaned DataFrame again (I'll cheat and show you a few rows down, so you can see the next issue, which is what [`iloc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html) is doing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438870c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[42:48]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebebf28",
   "metadata": {},
   "source": [
    "So we have some bad lines to remove. I suggest dropping them by requiring one of the columns to be non-missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['User ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f937ec9",
   "metadata": {},
   "source": [
    "Lastly, we have \"data types\" to fix. If you look at the `dtypes` attribute, you'll see that the `Post Date` is an `object`, which means it's a string, not a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17379f8e",
   "metadata": {},
   "source": [
    "But we know that these are actually a numeric value and date respectively, so we you should convert them to the appropriate types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Post Date'] = # write your own code (or ask AI) to make the Post Date column a date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a94930",
   "metadata": {},
   "source": [
    "Now, the last step is to produce the Lab's output, which is a screenshot of the cleaned table, and to list the unique `User ID` values. There are a few ways to do this, and if you put your cursor after `df['User ID']` and wait, Gemini might even suggest a solution for you (but only because I wrote that comment, if it weren't there, then it wouldn't know what you want. Remember this for future reference, it's *incredibly useful* to know that you can often just write a comment and get AI to give you the code to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5368be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own code to list the unique values:\n",
    "df['User ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f79aa",
   "metadata": {},
   "source": [
    "Now take a screenshot, and you're done! I'd save this notebook, because Homework 2 builds on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accountingdataanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
